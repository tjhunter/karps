{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1 - writing UDAFs the simple way\n",
    "\n",
    "This small example shows how simple it could be to write a UDAF in Spark with moderate additions to the existing API. It takes the example published in the [Databricks blog](https://databricks.com/blog/2015/09/16/apache-spark-1-5-dataframe-api-highlights.html) to add an operator for the [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean).\n",
    "\n",
    "Let's get done with some imports first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The main function\n",
    "import karps as ks\n",
    "# The standard library\n",
    "import karps.functions as f\n",
    "# Some tools to display the computation process:\n",
    "from karps.display import show_phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the definition of the harmonic mean, which is a simple function. Given a column containing floating point values, it is defined as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def harmonic_mean(col):\n",
    "    count = f.as_double(f.count(col))\n",
    "    inv_sum = 1.0/f.sum(1.0/col)\n",
    "    return inv_sum * count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly how one would want to code it in numpy, pandas, and using basic Spark constructs. In fact, you can run this code straight inside Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3333333333333333"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Pandas to evaluate our function:\n",
    "import pandas as pd\n",
    "pandas_df = pd.DataFrame([1.0, 2.0])\n",
    "harmonic_mean(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This code has a number of problems if you want to use it in Spark however:\n",
    " - reusability: this function works great on the column of a dataframe or of a column, but it cannot be reused with `groupby` for instance.\n",
    " - performance: most Spark tutorials will teach you that as it stands, this function has crappy performance. It will recompute the input twice, which may be very expensive in some cases.\n",
    "\n",
    "This is why if one wants to use it, it is immediately advised to use the `cache` function of Spark, which still requires all the data to stay materialized. Karps provides the convenient `autocache` operator which automatically decide if caching is appropriate. We are going to use it on this simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/my_input@org.spark.DistributedLiteral:double"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a HUGE dataframe\n",
    "df = ks.dataframe([1.0, 2.0], name=\"my_input\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/multiply6!org.spark.LocalStructuredTransform:double"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And apply our function:\n",
    "cached_df = f.autocache(df)\n",
    "hmean = harmonic_mean(cached_df)\n",
    "hmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Something to immediately note is that the computation is lazy: nothing gets computed and all you get is an object called `multiply6` of type `double`. Let's compute it. Thanks to lazy evaluation, the Karps compiler can rearrange the computations to make them run faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All computations happen within a session, which keeps track of the state in Spark.\n",
    "s = ks.session(\"demo1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `compute` function not only triggest the computation, but also provides more debugging information into what is happening. We are going to introspect the compiler passes to see how things get transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = s.compute(hmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the initial graph of computation, as we built it. Click on the nodes to have more detailed information.\n",
    "\n",
    "It is very clear that two computations are going to be run in parallel from the same dataset, and that caching will happen right before forking these computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1000px;height:620px;border:0\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.7144365456918119&quot;).pbtxt = 'node {\\n  name: &quot;my_input&quot;\\n  op: &quot;org.spark.DistributedLiteral&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;cell {  array_value {    values { double_value: 1.0 } values { double_value: 2.0 }  }}cell_type { array_type { basic_type: DOUBLE } }&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;autocache0&quot;\\n  op: &quot;org.spark.Autocache&quot;\\n  input: &quot;my_input&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;inverse3&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;autocache0&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  function {    function_name: .inverse.    inputs { extraction { } }    expected_type { basic_type: DOUBLE }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sum4&quot;\\n  op: &quot;org.spark.StructuredReduce&quot;\\n  input: &quot;inverse3&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;agg_op {  op {    function_name: .sum.    inputs { }    expected_type { basic_type: DOUBLE }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;inverse5&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;sum4&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  function {    function_name: .inverse.    inputs { extraction { } }    expected_type { basic_type: DOUBLE }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;count1&quot;\\n  op: &quot;org.spark.StructuredReduce&quot;\\n  input: &quot;autocache0&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;int&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;agg_op {  op {    function_name: .count. inputs { } expected_type { basic_type: INT }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;cast_double2&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;count1&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  function {    function_name: .cast_double.    inputs { extraction { } }    expected_type { basic_type: DOUBLE }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;multiply6&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;inverse5&quot;\\n  input: &quot;cast_double2&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  function {    function_name: .multiply.    inputs { broadcast { } }    inputs { broadcast { observable_index: 1 } }    expected_type { basic_type: DOUBLE }  }}&quot;\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.7144365456918119&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_phase(comp, \"initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1000px;height:620px;border:0\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.7769534106292852&quot;).pbtxt = 'node {\\n  name: &quot;my_input&quot;\\n  op: &quot;org.spark.DistributedLiteral&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;cell {  array_value {    values { double_value: 1.0 } values { double_value: 2.0 }  }}cell_type { array_type { basic_type: DOUBLE } }&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;autocache0&quot;\\n  op: &quot;org.spark.Autocache&quot;\\n  input: &quot;my_input&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;autocache0_ks_aggstruct&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;autocache0&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{_1:double _2:double}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields { extraction { } field_name: ._1. }    fields {      function {        function_name: .inverse.        inputs { extraction { } }        expected_type { basic_type: DOUBLE }      }      field_name: ._2.    }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sum4&quot;\\n  op: &quot;org.spark.StructuredReduce&quot;\\n  input: &quot;autocache0_ks_aggstruct&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;agg_op {  op {    function_name: .sum.    inputs { path: ._2. }    expected_type { basic_type: DOUBLE }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;inverse5&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;sum4&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  function {    function_name: .inverse.    inputs { extraction { } }    expected_type { basic_type: DOUBLE }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;count1&quot;\\n  op: &quot;org.spark.StructuredReduce&quot;\\n  input: &quot;autocache0_ks_aggstruct&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;int&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;agg_op {  op {    function_name: .count.    inputs { path: ._1. }    expected_type { basic_type: INT }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;cast_double2&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;count1&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  function {    function_name: .cast_double.    inputs { extraction { } }    expected_type { basic_type: DOUBLE }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;multiply6_karps_localpack&quot;\\n  op: &quot;org.spark.LocalPack&quot;\\n  input: &quot;inverse5&quot;\\n  input: &quot;cast_double2&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{_1:double _2:double}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;multiply6&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;multiply6_karps_localpack&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  function {    function_name: .multiply.    inputs { extraction { path: ._1. } }    inputs { extraction { path: ._2. } }    expected_type { basic_type: DOUBLE }  }}&quot;\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.7769534106292852&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_phase(comp, \"MERGE_PREAGG_AGGREGATIONS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important part to notice though is that after the `count1` and the `sum4` nodes, all the other nodes are observables (local values). They do not involve distributed datasets anymore, so they are very cheap to compute. The Karps compiler is going to optimize the distributed part to reduce the amount of computations, everything after that is not important for now.\n",
    "\n",
    "One of the first phases merges the `inverse3` node into a single lineage, and then fuses the aggregations `sum4` and `count1` into a single joint aggregation. If you look at the graph below, the new nodes `sum4` and `count1` are in fact dummy projections that operate on local data. All the hard work is being done in a new node with a horrible name: `autocache0_ks_aggstruct...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1000px;height:620px;border:0\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.6759935116233043&quot;).pbtxt = 'node {\\n  name: &quot;my_input&quot;\\n  op: &quot;org.spark.DistributedLiteral&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;cell {  array_value {    values { double_value: 1.0 } values { double_value: 2.0 }  }}cell_type { array_type { basic_type: DOUBLE } }&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;autocache0&quot;\\n  op: &quot;org.spark.Autocache&quot;\\n  input: &quot;my_input&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;autocache0_ks_aggstruct&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;autocache0&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{_1:double _2:double}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields { extraction { } field_name: ._1. }    fields {      function {        function_name: .inverse.        inputs { extraction { } }        expected_type { basic_type: DOUBLE }      }      field_name: ._2.    }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;autocache0_ks_aggstruct_karps_merged_agg&quot;\\n  op: &quot;org.spark.StructuredReduce&quot;\\n  input: &quot;autocache0_ks_aggstruct&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{_1:int _2:double}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;agg_op {  struct {    fields {      op {        function_name: .count.        inputs { path: ._1. }        expected_type { basic_type: INT }      }      field_name: ._1.    }    fields {      op {        function_name: .sum.        inputs { path: ._2. }        expected_type { basic_type: DOUBLE }      }      field_name: ._2.    }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sum4&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;autocache0_ks_aggstruct_karps_merged_agg&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op { extraction { path: ._2. } }&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;inverse5&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;sum4&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  function {    function_name: .inverse.    inputs { extraction { } }    expected_type { basic_type: DOUBLE }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;count1&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;autocache0_ks_aggstruct_karps_merged_agg&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;int&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op { extraction { path: ._1. } }&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;cast_double2&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;count1&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  function {    function_name: .cast_double.    inputs { extraction { } }    expected_type { basic_type: DOUBLE }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;multiply6_karps_localpack&quot;\\n  op: &quot;org.spark.LocalPack&quot;\\n  input: &quot;inverse5&quot;\\n  input: &quot;cast_double2&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{_1:double _2:double}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;multiply6&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;multiply6_karps_localpack&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  function {    function_name: .multiply.    inputs { extraction { path: ._1. } }    inputs { extraction { path: ._2. } }    expected_type { basic_type: DOUBLE }  }}&quot;\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.6759935116233043&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_phase(comp, \"MERGE_AGGREGATIONS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we only perform a single aggregation, do we still need to cache the data? We don't! The next compiler phase is going to inspect the autocache nodes, and see how many times they get to be aggregated, and remove them if possible. In this case, it correctly infers that we do not need this `autocache0` operator. Here is the final graph that gets executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1000px;height:620px;border:0\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.9212716584154327&quot;).pbtxt = 'node {\\n  name: &quot;my_input&quot;\\n  op: &quot;org.spark.DistributedLiteral&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;cell {  array_value {    values { double_value: 1.0 } values { double_value: 2.0 }  }}cell_type { array_type { basic_type: DOUBLE } }&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;autocache0_ks_aggstruct&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;my_input&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{_1:double _2:double}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields { extraction { } field_name: ._1. }    fields {      function {        function_name: .inverse.        inputs { extraction { } }        expected_type { basic_type: DOUBLE }      }      field_name: ._2.    }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;autocache0_ks_aggstruct_karps_merged_agg&quot;\\n  op: &quot;org.spark.StructuredReduce&quot;\\n  input: &quot;autocache0_ks_aggstruct&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{_1:int _2:double}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;agg_op {  struct {    fields {      op {        function_name: .count.        inputs { path: ._1. }        expected_type { basic_type: INT }      }      field_name: ._1.    }    fields {      op {        function_name: .sum.        inputs { path: ._2. }        expected_type { basic_type: DOUBLE }      }      field_name: ._2.    }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sum4&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;autocache0_ks_aggstruct_karps_merged_agg&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op { extraction { path: ._2. } }&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;inverse5&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;sum4&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  function {    function_name: .inverse.    inputs { extraction { } }    expected_type { basic_type: DOUBLE }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;count1&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;autocache0_ks_aggstruct_karps_merged_agg&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;int&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op { extraction { path: ._1. } }&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;cast_double2&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;count1&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  function {    function_name: .cast_double.    inputs { extraction { } }    expected_type { basic_type: DOUBLE }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;multiply6_karps_localpack&quot;\\n  op: &quot;org.spark.LocalPack&quot;\\n  input: &quot;inverse5&quot;\\n  input: &quot;cast_double2&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{_1:double _2:double}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;multiply6&quot;\\n  op: &quot;org.spark.LocalStructuredTransform&quot;\\n  input: &quot;multiply6_karps_localpack&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;double&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  function {    function_name: .multiply.    inputs { extraction { path: ._1. } }    inputs { extraction { path: ._2. } }    expected_type { basic_type: DOUBLE }  }}&quot;\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.9212716584154327&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_phase(comp, \"final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More work could be done to simplify the local nodes, but this is outside the scope of this first project.\n",
    "\n",
    "As a conclusion, we wrote some minimalistic, poorly performing code in python. Karps turned it into high-performance operations that can then be optimized easily by the Spark SQL engine. In fact, this code in practice is faster than a UDAF because it can directly understood by Tungsten. In addition, this function can be reused inside aggregations with no change, as we will see.\n",
    "\n",
    "As a summary, karps lets you write the code you want to write, and turns it into a program that is:\n",
    " - faster (sometimes as fast or faster than manually crafted code)\n",
    " - reusable and easy to compose\n",
    " - easy to introspect thanks to tensorboard\n",
    " - easy to test independently\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to get the actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(double, double_value: 1.3333333333333333\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in short if you do not want to see what is happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3333333333333333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.eval(hmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
